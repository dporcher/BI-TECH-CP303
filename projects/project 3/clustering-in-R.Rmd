---
title: "Cluster Analysis in R"
output:
  html_document:
    theme: readable
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: 2
date: "June 6, 2016"
---

```{r setup, include = FALSE}
# my set-up
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
                      fig.align = 'center')
 
options(scipen = 999)

require(dplyr)
require(ggplot2)

colleges = read.delim('./data/colleges.tsv',
                      sep = '\t',
                      header = TRUE)
```

# Read in data

We're continuing our exploration of college features, so read in the College
Scorecard data:

```{r, eval = FALSE}
require(dplyr)
require(ggplot2)

colleges = read.delim('./data/colleges.tsv',
                      sep = '\t',
                      header = TRUE)
```

# Preprocessing

Most clustering techniques work best when the data are centered and scaled.
Recall that a variable is centered when you subtract its mean from each 
observation. A variable is scaled when you divide each observation by 
the variable standard deviation. When we center and scale a variable, the 
resulting variable follows a z-distribution with mean = 0 and sd = 1.

```{r}
# set the seed
set.seed(100)

# remove rows containing missing data
#catalog = na.omit(catalog)
```

Before we can scale the data, we need to convert the product group from a factor
into a numeric value:

```{r}
# convert the group into dummy variables
# group_dummies = model.matrix(~ factor(sub$group) - 1)
# colnames(group_dummies) = c('book', 'dvd', 'music', 'video')

# append the dummy variables back on and drop the 'group' variable
# sub = cbind(sub, group_dummies)
# sub$group = NULL

# scale the data 
# scaled = as.data.frame(scale(select(sub, -labels)))
# scaled$labels = sub$labels
```

# K-means clustering

We have 4 product types, so maybe K = 4 is a good place to start?

```{r}  
# restrict the data to numerical variables
numeric_variables = sapply(colleges, is.numeric)

college_features = colleges[ , numeric_variables]

# remove missing values
college_features = na.omit(college_features)

id = college_features$id
college_features$id = NULL

# run k-means clustering
kmeans_cluster = kmeans(college_features, 4)

# check what attributes are in the kmeans object
attributes(kmeans_cluster)

# Find which cluster the observations belong to
head(kmeans_cluster$cluster, 10)

# centers
kmeans_cluster$centers

# plot 4 clusters
ggplot(college_features, 
       aes(x = median_debt, 
           y = median_earnings, 
           color = factor(kmeans_cluster$cluster))) +
  geom_point(alpha = 0.50) +
  theme_minimal() 

with(college_features, pairs(select(college_features, engineering_major_perc, median_earnings, family_income_median, median_debt), col = c(1:4)[kmeans_cluster$cluster])) 
```

# Hierarchical Clustering

```{r}
# compute the euclidean distance
euclidean = dist(college_features, method = 'euclidean')

# attributes
attributes(euclidean)

# hierarchical clustering
hier = hclust(euclidean)

# label by id
hier$labels = id

# plot dendrogram
plot(hier)
```